

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Publications - David Martin</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="David Martin">
<meta property="og:title" content="Publications">


  <link rel="canonical" href="http://localhost:4000/publications/">
  <meta property="og:url" content="http://localhost:4000/publications/">







  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "David Martin",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="David Martin Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:4000/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:4000/images/favicon-16x16.png">
<link rel="manifest" href="http://localhost:4000/images/site.webmanifest">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="http://localhost:4000/images/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">David Martin</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/press/">Press</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000">Talks and Demos</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000">Resume/CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  



  <div class="archive">
    
      <h1 class="page__title">Publications</h1>
    
    




<!-- <p class="page__taxonomy">
  <details open>
    <summary>Filter by category</summary>
    
      <button class="page__taxonomy-item category-filter-item" category="Dataset"> Dataset </button>
    
      <button class="page__taxonomy-item category-filter-item" category="demo"> demo </button>
    
      <button class="page__taxonomy-item category-filter-item" category="poster"> poster </button>
    
  </details>
  <details>
    <summary>Filter by tag</summary>
    
      <button class="page__taxonomy-item tag-filter-item" tag="accessibility"> accessibility </button>
    
      <button class="page__taxonomy-item tag-filter-item" tag="sensing"> sensing </button>
    
      <button class="page__taxonomy-item tag-filter-item" tag="subtle-interaction"> subtle-interaction </button>
    
  </details>
</p> -->


  
  
    <h2 id="2024" class="archive__subtitle">2024</h2>
    
  
  





<div class="list__item" title="Towards Improving Real-Time Head-Worn Display Caption Mediated Conversations with Speaker Feedback for Hearing Conversation Partners" id="towards-improving-real-time-head-worn-display-caption-mediated-conversations-with-speaker-feedback-for-hearing-conversation-partners" year="2024">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    <!--  -->

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/files/papers/SpeakerFeedback_CHI_2024.pdf">Towards Improving Real-Time Head-Worn Display Caption Mediated Conversations with Speaker Feedback for Hearing Conversation Partners
</a>
      
    </h2>
    
    <!-- 
      <img src="/images/speakerfeedback.jpg" alt="Towards Improving Real-Time Head-Worn Display Caption Mediated Conversations with Speaker Feedback for Hearing Conversation Partners" align="left" style="padding: 10px; width: 30%;">
     -->
    
    <!--  -->

    
      <p>Jenna Kang, Emily Layton, <b>David Martin</b>, Thad Starner</p>
      <p>Published in <i>CHI EA '24: Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</i>, 2024 </p>
      <details>
        <summary>Abstract</summary>
        <p>Many products attempt to provide captioning for Deaf and Hard-of-Hearing individuals through smart glasses using automatic speech recognition. Yet there still remain challenges due to system delays and dropouts, heavy accents, and general mistranscriptions. Due to the imperfections of automatic speech recognition, there remains conversational difficulties for Deaf and Hard-of-Hearing individuals when conversing with hearing individuals. For instance, hearing conversation partners may often not realize that their Deaf or Hard-of-Hearing conversation partner is missing parts of the conversation. This study examines whether providing visual feedback of captioned conversation to hearing conversation partners can enhance conversational accuracy and dynamics. Through a task-based experiment involving 20 hearing participants we measure the impact on visual feedback of captioning on error rates, self-corrections, and subjective workloads. Our findings indicate that when given visual feedback, the average number of errors made by participants was 1.15 less (p = 0.00258) indicating a notable reduction in errors. When visual feedback is provided, the average number of self-corrections increased by 3.15 (p < 0.001), suggesting a smoother and more streamlined conversation These results show that the inclusion of visual feedback in conversation with a Deaf or Hard-of-Hearing individual can lead to improved conversational efficiency.</p>
      </details>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    


    <!--  -->
    <!-- 
    
     -->
    <!-- <p class="page__taxonomy">
      
        <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i></strong>
        <span itemprop="keywords">
        
          
          
          <a href="?category=poster" style="text-decoration: none" class="page__taxonomy-item"> poster</a>
        
        </span>
        &nbsp;&nbsp;&nbsp;&nbsp;
      
      <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i></strong>
      <span itemprop="keywords">
      
      </span>
      
        <a href="https://doi.org/10.1145/3597638.3614491" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw fas fa-quote-right"></i>
            
          doi</a>
      
        <a href="/files/papers/SpeakerFeedback_CHI_2024.pdf" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw far fa-file-pdf"></i>
            
          paper</a>
      
    </p> -->

    <!-- <p class="archive__item-links">doidoihttps://doi.org/10.1145/3597638.3614491paperpdf/files/papers/SpeakerFeedback_CHI_2024.pdf</p> -->
  </article>
</div>


  
  
    <h2 id="2023" class="archive__subtitle">2023</h2>
    
  
  





<div class="list__item" title="PopSign ASL v1. 0: An Isolated American Sign Language Dataset Collected via Smartphones" id="popsign-asl-v1-0-an-isolated-american-sign-language-dataset-collected-via-smartphones" year="2023">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    <!--  -->

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/files/papers/PopSign_NeurIPS_2024.pdf">PopSign ASL v1. 0: An Isolated American Sign Language Dataset Collected via Smartphones
</a>
      
    </h2>
    
    <!-- 
      <img src="/images/fingerspeller.png" alt="PopSign ASL v1. 0: An Isolated American Sign Language Dataset Collected via Smartphones" align="left" style="padding: 10px; width: 30%;">
     -->
    
    <!--  -->

    
      <p>Thad Starner, Sean Forbes, Matthew So, <b>David Martin</b>, Rohit Sridhar, Gururaj Deshpande, Sam Sepah, Sahir Shahryar, Khushi Bhardwaj, Tyler Kwok, Daksh Sehgal, Saad Hassan, Bill Neubauer, Sofia Vempala, Alec Tan, Jocelyn Heath, Unnathi Kumar, Priyanka Mosur, Tavenner Hall, Rajandeep Singh, Christopher Cui, Glenn Cameron, Sohier Dane, Garrett Tanzer</p>
      <p>Published in <i>Thirty-seventh Conference on Neural Information Processing Systems</i>, 2023 </p>
      <details>
        <summary>Abstract</summary>
        <p>PopSign is a smartphone-based bubble-shooter game that helps hearing parentsof deaf infants learn sign language. To help parents practice their ability to sign,PopSign is integrating sign language recognition as part of its gameplay. Fortraining the recognizer, we introduce the PopSign ASL v1.0 dataset that collectsexamples of 250 isolated American Sign Language (ASL) signs using Pixel 4Asmartphone selfie cameras in a variety of environments. It is the largest publiclyavailable, isolated sign dataset by number of examples and is the first dataset tofocus on one-handed, smartphone signs. We collected over 210,000 examplesat 1944x2592 resolution made by 47 consenting Deaf adult signers for whomAmerican Sign Language is their primary language. We manually reviewed 217,866of these examples, of which 175,023 (approximately 700 per sign) were the signintended for the educational game. 39,304 examples were recognizable as a signbut were not the desired variant or were a different sign. We provide a training setof 31 signers, a validation set of eight signers, and a test set of eight signers. Abaseline LSTM model for the 250-sign vocabulary achieves 82.1% accuracy (81.9%class-weighted F1 score) on the validation set and 84.2% (83.9% class-weightedF1 score) on the test set. Gameplay suggests that accuracy will be sufficient forcreating educational games involving sign language recognition.</p>
      </details>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    


    <!--  -->
    <!-- 
    
     -->
    <!-- <p class="page__taxonomy">
      
        <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i></strong>
        <span itemprop="keywords">
        
          
          
          <a href="?category=Dataset" style="text-decoration: none" class="page__taxonomy-item"> Dataset</a>
        
        </span>
        &nbsp;&nbsp;&nbsp;&nbsp;
      
      <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i></strong>
      <span itemprop="keywords">
      
      </span>
      
        <a href="https://doi.org/10.1145/3597638.3614491" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw fas fa-quote-right"></i>
            
          doi</a>
      
        <a href="/files/papers/PopSign_NeurIPS_2024.pdf" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw far fa-file-pdf"></i>
            
          paper</a>
      
    </p> -->

    <!-- <p class="archive__item-links">doidoihttps://doi.org/10.1145/3597638.3614491paperpdf/files/papers/PopSign_NeurIPS_2024.pdf</p> -->
  </article>
</div>


  
  
  





<div class="list__item" title="FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition" id="fingerspeller-camera-free-text-entry-using-smart-rings-for-american-sign-language-fingerspelling-recognition" year="2023">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    <!--  -->

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/files/papers/FingerSpeller_ASSETS_2023_Poster.pdf">FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition
</a>
      
    </h2>
    
    <!-- 
      <img src="/images/fingerspeller.png" alt="FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition" align="left" style="padding: 10px; width: 30%;">
     -->
    
    <!--  -->

    
      <p><b>David Martin</b>, Zikang Leng, Tan Gemicioglu, Jon Womack, Jocelyn Heath, Bill Neubauer, Hyeokhyen Kwon, Thomas Plöetz, Thad Starner</p>
      <p>Published in <i>The 25th International ACM SIGACCESS Conference on Computers and Accessibility</i>, 2023 </p>
      <details>
        <summary>Abstract</summary>
        <p>Camera-based text entry using American Sign Language (ASL) fingerspelling has become more feasible due to recent advancements in recognition technology. However, there are numerous situations where camera-based text entry may not be ideal or acceptable. To address this, we present FingerSpeller, a solution that enables camera-free text entry using smart rings. FingerSpeller utilizes accelerometers embedded in five smart rings from TapStrap, a commercially available wearable keyboard, to track finger motion and recognize fingerspelling. A Hidden Markov Model (HMM) based backend with continuous Gaussian modeling facilitates accurate recognition as evaluated in a real-world deployment. In offline isolated word recognition experiments conducted on a 1,164-word dictionary, FingerSpeller achieves an average character accuracy of 91% and word accuracy of 87% across three participants. Furthermore, we demonstrate that the system can be downsized to only two rings while maintaining an accuracy level of approximately 90% compared to the original configuration. This reduction in form factor enhances user comfort and significantly improves the overall usability of the system.</p>
      </details>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    


    <!--  -->
    <!-- 
    
     -->
    <!-- <p class="page__taxonomy">
      
        <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i></strong>
        <span itemprop="keywords">
        
          
          
          <a href="?category=poster" style="text-decoration: none" class="page__taxonomy-item"> poster</a>
        
        </span>
        &nbsp;&nbsp;&nbsp;&nbsp;
      
      <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i></strong>
      <span itemprop="keywords">
      
        
        
        <a href="?tags=accessibility" style="text-decoration: none" class="page__taxonomy-item">accessibility</a><span class="sep">, </span>
      
        
        
        <a href="?tags=sensing" style="text-decoration: none" class="page__taxonomy-item">sensing</a><span class="sep">, </span>
      
        
        
        <a href="?tags=subtle-interaction" style="text-decoration: none" class="page__taxonomy-item">subtle-interaction</a>
      
      </span>
      
        <a href="https://doi.org/10.1145/3597638.3614491" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw fas fa-quote-right"></i>
            
          doi</a>
      
        <a href="/files/papers/FingerSpeller_ASSETS_2023_Poster.pdf" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw far fa-file-pdf"></i>
            
          paper</a>
      
    </p> -->

    <!-- <p class="archive__item-links">doidoihttps://doi.org/10.1145/3597638.3614491paperpdf/files/papers/FingerSpeller_ASSETS_2023_Poster.pdf</p> -->
  </article>
</div>


  
  
  





<div class="list__item" title="ToozKit: System for Experimenting with Captions on a Head-worn Display" id="toozkit-system-for-experimenting-with-captions-on-a-head-worn-display" year="2023">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    <!--  -->

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/files/papers/ToozKit_ISWC_2023.pdf">ToozKit: System for Experimenting with Captions on a Head-worn Display
</a>
      
    </h2>
    
    <!-- 
      <img src="/images/fingerspeller.png" alt="ToozKit: System for Experimenting with Captions on a Head-worn Display" align="left" style="padding: 10px; width: 30%;">
     -->
    
    <!--  -->

    
      <p>Peter Feng, <b>David Martin</b>, Thad Starner</p>
      <p>Published in <i>UbiComp/ISWC '23 Adjunct: Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing & the 2023 ACM International Symposium on Wearable Computing</i>, 2023 </p>
      <details>
        <summary>Abstract</summary>
        <p>The advent of Automatic Speech Recognition (ASR) has made real-time captioning for the Deaf and Hard-of-Hearing (DHH) community possible, and integration of ASR into Head-worn Displays (HWD) is gaining momentum. We propose a demonstration of an open source, Android-based, captioning toolkit intended to help researchers and early adopters more easily develop interfaces and test usability. Attendees will briefly learn about the the technical architecture, use-cases and features of the toolkit as well as have the opportunity to experience using the captioning glasses on the tooz HWD while engaging in conversation with the demonstrators.</p>
      </details>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    


    <!--  -->
    <!-- 
    
     -->
    <!-- <p class="page__taxonomy">
      
        <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i></strong>
        <span itemprop="keywords">
        
          
          
          <a href="?category=demo" style="text-decoration: none" class="page__taxonomy-item"> demo</a>
        
        </span>
        &nbsp;&nbsp;&nbsp;&nbsp;
      
      <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i></strong>
      <span itemprop="keywords">
      
      </span>
      
        <a href="https://doi.org/10.1145/3597638.3614491" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw fas fa-quote-right"></i>
            
          doi</a>
      
        <a href="/files/papers/ToozKit_ISWC_2023.pdf" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw far fa-file-pdf"></i>
            
          paper</a>
      
    </p> -->

    <!-- <p class="archive__item-links">doidoihttps://doi.org/10.1145/3597638.3614491paperpdf/files/papers/ToozKit_ISWC_2023.pdf</p> -->
  </article>
</div>


  
  
    <h2 id="2022" class="archive__subtitle">2022</h2>
    
  
  





<div class="list__item" title="Preferences for Captioning on Emulated Head Worn Displays While in Group Conversation" id="preferences-for-captioning-on-emulated-head-worn-displays-while-in-group-conversation" year="2022">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    <!--  -->

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/files/papers/GroupConvo_ISWC_2022.pdf">Preferences for Captioning on Emulated Head Worn Displays While in Group Conversation
</a>
      
    </h2>
    
    <!-- 
      <img src="/images/groupconvo.png" alt="Preferences for Captioning on Emulated Head Worn Displays While in Group Conversation" align="left" style="padding: 10px; width: 30%;">
     -->
    
    <!--  -->

    
      <p>Gabriel Britian, <b>David Martin</b>, Tyler Kwok, Adam Sumilong, Thad Starner</p>
      <p>Published in <i>ISWC '22: Proceedings of the 2022 ACM International Symposium on Wearable Computers</i>, 2022 </p>
      <details>
        <summary>Abstract</summary>
        <p>Head worn displays (HWDs) can provide a discreet method of captioning for people who are d/Deaf or hard of hearing (DHH); however, group conversations remain a difficult scenario as the wearer has difficulty in determining who is speaking and where to look. Using an HWD emulator during a group conversation, we compare eight DHH users’ perceptions of four conditions: an 80 degree field-of-view (FOV) HWD that pins captioning text to each speaker (Registered), a HWD where the captioning remains in the same place in the user’s visual field (Non-registered), Non-registered plus indicators as to which direction the current speaker is relative to the user’s line of sight (Indicators), and a control of captions displayed on a Phone. Preference increased in order of Phone, Non-registered, Indicators, and Registered. While an 80 degree FOV HWD is not practical to create in a pair of normal looking eyeglasses, pilot testing with 12 hearing participants suggests a FOV between 20 and 30 degrees might be sufficient.</p>
      </details>
    

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    


    <!--  -->
    <!-- 
    
     -->
    <!-- <p class="page__taxonomy">
      
        <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i></strong>
        <span itemprop="keywords">
        
          
          
          <a href="?category=poster" style="text-decoration: none" class="page__taxonomy-item"> poster</a>
        
        </span>
        &nbsp;&nbsp;&nbsp;&nbsp;
      
      <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i></strong>
      <span itemprop="keywords">
      
      </span>
      
        <a href="https://doi.org/10.1145/3597638.3614491" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw fas fa-quote-right"></i>
            
          doi</a>
      
        <a href="/files/papers/GroupConvo_ISWC_2022.pdf" style="text-decoration: none" class="page__taxonomy-item">
          
              <i class="fa fa-fw far fa-file-pdf"></i>
            
          paper</a>
      
    </p> -->

    <!-- <p class="archive__item-links">doidoihttps://doi.org/10.1145/3597638.3614491paperpdf/files/papers/GroupConvo_ISWC_2022.pdf</p> -->
  </article>
</div>



<!-- Credit to mmpataki for parts of this script -->

<script>

  var posts = {
    
      "Towards Improving Real-Time Head-Worn Display Caption Mediated Conversations with Speaker Feedback for Hearing Conversation Partners": {
        category: "poster",
        tags: [
          
          
        ]
      },
    
      "PopSign ASL v1. 0: An Isolated American Sign Language Dataset Collected via Smartphones": {
        category: "Dataset",
        tags: [
          
          
        ]
      },
    
      "FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition": {
        category: "poster",
        tags: [
          
          
            
            
            "sensing"
          
            ,
            
            "accessibility"
          
            ,
            
            "subtle-interaction"
          
        ]
      },
    
      "ToozKit: System for Experimenting with Captions on a Head-worn Display": {
        category: "demo",
        tags: [
          
          
        ]
      },
    
      "Preferences for Captioning on Emulated Head Worn Displays While in Group Conversation": {
        category: "poster",
        tags: [
          
          
        ]
      },
    
  };
  
  var categories = {
    
    
        
        
        "Dataset": {
          selected: false
        }
    
        ,
        
        "demo": {
          selected: false
        }
    
        ,
        
        "poster": {
          selected: false
        }
    
  };
  
  var tags = {
    
    
        
        
        "accessibility": {
          selected: false
        }
    
        ,
        
        "sensing": {
          selected: false
        }
    
        ,
        
        "subtle-interaction": {
          selected: false
        }
    
  };
  
  var year_elements = document.getElementsByClassName("archive__subtitle");
  var post_elements = document.getElementsByClassName("list__item");

  var category_selected = "";
  var tag_selected = [];

  try {
    var category_temp = new URL(location).searchParams.get("category");
    if(category_temp != null) {
      category_selected = category_temp;
    }
  } catch {}
  try {
    var tag_temp = new URL(location).searchParams.get("tags").split(",");
    if(tag_temp != null) {
      tag_selected = tag_temp;
    }
  } catch {}

var category_selected_element = null;
  function categoryClicked(element) {
    if(element.getAttribute("category") != category_selected && category_selected.length != 0 && categories[category_selected].selected == true) {
      category_selected_element.classList.toggle("page__taxonomy-item-selected");
      categories[category_selected].selected = !categories[category_selected].selected;
    }
    element.classList.toggle("page__taxonomy-item-selected");
    categories[element.getAttribute("category")].selected = !categories[element.getAttribute("category")].selected;
    if(categories[element.getAttribute("category")].selected) {
      category_selected = element.getAttribute("category");
      category_selected_element = element;
    } else {
      category_selected = "";
    }
    updatePosts();
  };
  var category_elements = document.getElementsByClassName("category-filter-item");
  for(var i = 0; i < category_elements.length; i++) {
    category_elements[i].addEventListener("click", function() {
      categoryClicked(this);
    });
    if(category_elements[i].getAttribute("category") == category_selected) {
      category_elements[i].click();
    }
  }
  
  var tag_selected_num = 0;
  function tagClicked(element) {
    element.classList.toggle("page__taxonomy-item-selected");
    tags[element.getAttribute("tag")].selected = !tags[element.getAttribute("tag")].selected;
    if(tags[element.getAttribute("tag")].selected) {
      tag_selected_num++;
    } else {
      tag_selected_num--;
    }
    updatePosts();
  };
  var tag_elements = document.getElementsByClassName("tag-filter-item");
  for(var i = 0; i < tag_elements.length; i++) {
    tag_elements[i].addEventListener("click", function() {
      tagClicked(this);
    });
    if(tag_selected.includes(tag_elements[i].getAttribute("tag"))) {
      tag_elements[i].click();
    }
  }

  function updatePosts() {
    for (post_element of post_elements) {
      var post = posts[post_element.getAttribute("title")];
      var show = true;
      if (post.category in categories) {
        show = show && categories[post.category].selected;
      }
      var show2 = false;
      for (tag of post.tags) {
        if (tag in tags) {
          show2 = show2 || tags[tag].selected;
        }
      }
      if ((category_selected.length === 0 || show) && (tag_selected_num == 0 || show2)) {
        post_element.style.display = "block";
      } else {
        post_element.style.display = "none";
      }
    }
    for (year_element of year_elements) {
      var show = false;
      for (post_element of post_elements) {
        if (post_element.style.display == "block" && post_element.getAttribute("year") == year_element.getAttribute("id")) {
          show = true;
        }
      }
      if (show) {
        year_element.style.display = "block";
      } else {
        year_element.style.display = "none";
      }
    }
  };
  
  updatePosts();
  </script>
  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!-- <a href="/sitemap/">Sitemap</a> -->
<!-- end custom footer snippets -->

        

<div class="page__footer-copyright">&copy; 2024 David Martin. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; adapted from <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>





  </body>
</html>

